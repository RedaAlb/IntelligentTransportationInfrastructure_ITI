{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets_utils import DatasetsUtils\n",
    "\n",
    "data_utils = DatasetsUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Licence plate detection (cropping the vehicles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 caltech_cars Done\n",
      "1 english_lp Done\n",
      "2 open_alpr_eu Done\n",
      "3 aolp Done\n",
      "This dataset is split into fixed train, test, and val sets, please provide an additional argument of\n",
      "subset=\"training|testing|validation\".\n",
      "4 ufpr_alpr Done\n"
     ]
    }
   ],
   "source": [
    "# Generating images of cropped vehicle patches and saving them alongside each image annotation file for all datasets.\n",
    "for i, dataset in enumerate(data_utils.datasets):\n",
    "    data_utils.generate_lp_det_data(i, \"vehicles_cropped\")\n",
    "    print(i, dataset, \"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the generation was successful.\n",
    "data_utils.check_darknet_yolov4_annos(\"vehicles_cropped/caltech_cars\", \"lp_class_name.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caltech_cars dataset done, files saved:\n",
      "vehicles_cropped/train_caltech_cars.txt\n",
      "vehicles_cropped/val_caltech_cars.txt\n",
      "vehicles_cropped/test_caltech_cars.txt\n",
      "\n",
      "english_lp dataset done, files saved:\n",
      "vehicles_cropped/train_english_lp.txt\n",
      "vehicles_cropped/val_english_lp.txt\n",
      "vehicles_cropped/test_english_lp.txt\n",
      "\n",
      "open_alpr_eu dataset done, files saved:\n",
      "vehicles_cropped/train_open_alpr_eu.txt\n",
      "vehicles_cropped/val_open_alpr_eu.txt\n",
      "vehicles_cropped/test_open_alpr_eu.txt\n",
      "\n",
      "aolp dataset done, files saved:\n",
      "vehicles_cropped/train_aolp.txt\n",
      "vehicles_cropped/val_aolp.txt\n",
      "vehicles_cropped/test_aolp.txt\n",
      "\n",
      "ufpr_alpr dataset done, files saved:\n",
      "vehicles_cropped/test_ufpr_alpr.txt\n",
      "vehicles_cropped/train_ufpr_alpr.txt\n",
      "vehicles_cropped/val_ufpr_alpr.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting datasets into train, val, and test sets.\n",
    "\n",
    "imgs_root = \"vehicles_cropped\"\n",
    "imgs_prefix = \"../../../../../datasets/vehicles_cropped\"\n",
    "\n",
    "for i, dataset in enumerate(data_utils.datasets):\n",
    "    data_utils.gen_dataset_split_files(i, imgs_root, imgs_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All subsets combined successfully, files_saved:\n",
      "vehicles_cropped/all_train.txt\n",
      "vehicles_cropped/all_val.txt\n",
      "vehicles_cropped/all_test.txt\n"
     ]
    }
   ],
   "source": [
    "# Combining all train, val, test sets of all datasets into three big subsets.\n",
    "data_utils.combine_subsets(\"vehicles_cropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Licence plate recognition (cropping the LP patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 caltech_cars Done\n",
      "1 english_lp Done\n",
      "2 open_alpr_eu Done\n",
      "3 aolp Done\n",
      "This dataset is split into fixed train, test, and val sets, please provide an additional argument of\n",
      "subset=\"training|testing|validation\".\n",
      "4 ufpr_alpr Done\n"
     ]
    }
   ],
   "source": [
    "# Generating images of cropped vehicle patches and saving them alongside each image annotation file for all datasets.\n",
    "for i, dataset in enumerate(data_utils.datasets):\n",
    "    data_utils.generate_lp_rec_data(i, \"lps_cropped\")\n",
    "    print(i, dataset, \"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the generation was successful.\n",
    "data_utils.check_darknet_yolov4_annos(\"lps_cropped/ufpr_alpr/training\", \"char_class_names.txt\")\n",
    "# data_utils.check_darknet_yolov4_annos(\"lps_cropped/caltech_cars\", \"char_class_names.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caltech_cars dataset done, files saved:\n",
      "lps_cropped/train_caltech_cars.txt\n",
      "lps_cropped/val_caltech_cars.txt\n",
      "lps_cropped/test_caltech_cars.txt\n",
      "\n",
      "english_lp dataset done, files saved:\n",
      "lps_cropped/train_english_lp.txt\n",
      "lps_cropped/val_english_lp.txt\n",
      "lps_cropped/test_english_lp.txt\n",
      "\n",
      "open_alpr_eu dataset done, files saved:\n",
      "lps_cropped/train_open_alpr_eu.txt\n",
      "lps_cropped/val_open_alpr_eu.txt\n",
      "lps_cropped/test_open_alpr_eu.txt\n",
      "\n",
      "aolp dataset done, files saved:\n",
      "lps_cropped/train_aolp.txt\n",
      "lps_cropped/val_aolp.txt\n",
      "lps_cropped/test_aolp.txt\n",
      "\n",
      "ufpr_alpr dataset done, files saved:\n",
      "lps_cropped/test_ufpr_alpr.txt\n",
      "lps_cropped/train_ufpr_alpr.txt\n",
      "lps_cropped/val_ufpr_alpr.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting datasets into train, val, and test sets.\n",
    "\n",
    "imgs_root = \"lps_cropped\"\n",
    "imgs_prefix = \"../../../../../datasets/lps_cropped\"\n",
    "\n",
    "for i, dataset in enumerate(data_utils.datasets):\n",
    "    data_utils.gen_dataset_split_files(i, imgs_root, imgs_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All subsets combined successfully, files_saved:\n",
      "lps_cropped/all_train.txt\n",
      "lps_cropped/all_val.txt\n",
      "lps_cropped/all_test.txt\n"
     ]
    }
   ],
   "source": [
    "# Combining all train, val, test sets of all datasets into three big subsets.\n",
    "data_utils.combine_subsets(\"lps_cropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Licence plate recognition (including negative images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 caltech_cars Done\n",
      "1 english_lp Done\n",
      "2 open_alpr_eu Done\n",
      "3 aolp Done\n",
      "This dataset is split into fixed train, test, and val sets, please provide an additional argument of\n",
      "subset=\"training|testing|validation\".\n",
      "4 ufpr_alpr Done\n"
     ]
    }
   ],
   "source": [
    "# Doubling the # of samples by using the negative of each LP patch.\n",
    "for i, dataset in enumerate(data_utils.datasets):\n",
    "    data_utils.generate_lp_rec_data(i, \"lps_cropped_neg\", negative_imgs=True)\n",
    "    print(i, dataset, \"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the generation was successful.\n",
    "data_utils.check_darknet_yolov4_annos(\"lps_cropped_neg/caltech_cars\", \"char_class_names.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caltech_cars dataset done, files saved:\n",
      "lps_cropped_neg/train_caltech_cars.txt\n",
      "lps_cropped_neg/val_caltech_cars.txt\n",
      "lps_cropped_neg/test_caltech_cars.txt\n",
      "\n",
      "english_lp dataset done, files saved:\n",
      "lps_cropped_neg/train_english_lp.txt\n",
      "lps_cropped_neg/val_english_lp.txt\n",
      "lps_cropped_neg/test_english_lp.txt\n",
      "\n",
      "open_alpr_eu dataset done, files saved:\n",
      "lps_cropped_neg/train_open_alpr_eu.txt\n",
      "lps_cropped_neg/val_open_alpr_eu.txt\n",
      "lps_cropped_neg/test_open_alpr_eu.txt\n",
      "\n",
      "aolp dataset done, files saved:\n",
      "lps_cropped_neg/train_aolp.txt\n",
      "lps_cropped_neg/val_aolp.txt\n",
      "lps_cropped_neg/test_aolp.txt\n",
      "\n",
      "ufpr_alpr dataset done, files saved:\n",
      "lps_cropped_neg/test_ufpr_alpr.txt\n",
      "lps_cropped_neg/train_ufpr_alpr.txt\n",
      "lps_cropped_neg/val_ufpr_alpr.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting datasets into train, val, and test sets.\n",
    "\n",
    "imgs_root = \"lps_cropped_neg\"\n",
    "imgs_prefix = \"../../../../../datasets/lps_cropped_neg\"\n",
    "\n",
    "for i, dataset in enumerate(data_utils.datasets):\n",
    "    data_utils.gen_dataset_split_files(i, imgs_root, imgs_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All subsets combined successfully, files_saved:\n",
      "lps_cropped_neg/all_train.txt\n",
      "lps_cropped_neg/all_val.txt\n",
      "lps_cropped_neg/all_test.txt\n"
     ]
    }
   ],
   "source": [
    "# Combining all train, val, test sets of all datasets into three big subsets.\n",
    "data_utils.combine_subsets(\"lps_cropped_neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For vehicle detection (ALPR datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 caltech_cars Done\n",
      "1 english_lp Done\n",
      "2 open_alpr_eu Done\n",
      "3 aolp Done\n",
      "This dataset is split into fixed train, test, and val sets, please provide an additional argument of\n",
      "subset=\"training|testing|validation\".\n",
      "4 ufpr_alpr Done\n"
     ]
    }
   ],
   "source": [
    "# Generating vehicle detection annotation files alongside each sample image for all datasets.\n",
    "for i, dataset in enumerate(data_utils.datasets):\n",
    "    data_utils.generate_v_det_data(i, \"vehicle_det\")\n",
    "    print(i, dataset, \"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the generation was successful.\n",
    "data_utils.check_darknet_yolov4_annos(\"vehicle_det/caltech_cars/\", \"v_det_class_name.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caltech_cars dataset done, files saved:\n",
      "vehicle_det/train_caltech_cars.txt\n",
      "vehicle_det/val_caltech_cars.txt\n",
      "vehicle_det/test_caltech_cars.txt\n",
      "\n",
      "english_lp dataset done, files saved:\n",
      "vehicle_det/train_english_lp.txt\n",
      "vehicle_det/val_english_lp.txt\n",
      "vehicle_det/test_english_lp.txt\n",
      "\n",
      "open_alpr_eu dataset done, files saved:\n",
      "vehicle_det/train_open_alpr_eu.txt\n",
      "vehicle_det/val_open_alpr_eu.txt\n",
      "vehicle_det/test_open_alpr_eu.txt\n",
      "\n",
      "aolp dataset done, files saved:\n",
      "vehicle_det/train_aolp.txt\n",
      "vehicle_det/val_aolp.txt\n",
      "vehicle_det/test_aolp.txt\n",
      "\n",
      "ufpr_alpr dataset done, files saved:\n",
      "vehicle_det/test_ufpr_alpr.txt\n",
      "vehicle_det/train_ufpr_alpr.txt\n",
      "vehicle_det/val_ufpr_alpr.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting datasets into train, val, and test sets.\n",
    "\n",
    "imgs_root = \"vehicle_det\"\n",
    "imgs_prefix = \"../../../../../datasets/vehicle_det\"\n",
    "\n",
    "for i, dataset in enumerate(data_utils.datasets):\n",
    "    data_utils.gen_dataset_split_files(i, imgs_root, imgs_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All subsets combined successfully, files_saved:\n",
      "vehicle_det/all_train.txt\n",
      "vehicle_det/all_val.txt\n",
      "vehicle_det/all_test.txt\n"
     ]
    }
   ],
   "source": [
    "# Combining all train, val, test sets of all datasets into three big subsets.\n",
    "data_utils.combine_subsets(\"vehicle_det\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For vehicle detection (COCO dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.36s)\n",
      "creating index...\n",
      "index created!\n",
      "COCO categories: \n",
      "person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush\n",
      "\n",
      "Class: car\n",
      "Class ID: 3\n",
      "Number of imgs: 12251\n",
      "Example img: {'license': 1, 'file_name': '000000360447.jpg', 'coco_url': 'http://images.cocodataset.org/train2017/000000360447.jpg', 'height': 480, 'width': 640, 'date_captured': '2013-11-22 22:58:31', 'flickr_url': 'http://farm3.staticflickr.com/2369/1512283194_26500b777d_z.jpg', 'id': 360447} \n",
      "\n",
      "Class: motorcycle\n",
      "Class ID: 4\n",
      "Number of imgs: 3502\n",
      "Example img: {'license': 5, 'file_name': '000000319487.jpg', 'coco_url': 'http://images.cocodataset.org/train2017/000000319487.jpg', 'height': 480, 'width': 640, 'date_captured': '2013-11-15 20:58:21', 'flickr_url': 'http://farm8.staticflickr.com/7111/7027597011_5f967e95be_z.jpg', 'id': 319487} \n",
      "\n",
      "Class: bus\n",
      "Class ID: 6\n",
      "Number of imgs: 3952\n",
      "Example img: {'license': 4, 'file_name': '000000466942.jpg', 'coco_url': 'http://images.cocodataset.org/train2017/000000466942.jpg', 'height': 480, 'width': 640, 'date_captured': '2013-11-16 21:10:40', 'flickr_url': 'http://farm4.staticflickr.com/3706/9244270714_a16969b289_z.jpg', 'id': 466942} \n",
      "\n",
      "Class: truck\n",
      "Class ID: 8\n",
      "Number of imgs: 6127\n",
      "Example img: {'license': 1, 'file_name': '000000032760.jpg', 'coco_url': 'http://images.cocodataset.org/train2017/000000032760.jpg', 'height': 482, 'width': 640, 'date_captured': '2013-11-15 05:03:28', 'flickr_url': 'http://farm8.staticflickr.com/7018/6504071945_98ace458f1_z.jpg', 'id': 32760} \n",
      "\n",
      "Class IDs [3, 4, 6, 8]\n",
      "Total imgs: 25832 (including duplicates)\n",
      "Finished downloading all images and creating annotation files for ['car', 'motorcycle', 'bus', 'truck'] classes in\n",
      " coco/coco_train\n"
     ]
    }
   ],
   "source": [
    "# Downloading the COCO images (training set)\n",
    "anno_file = \"coco/instances_train2017.json\"  # Get this from https://cocodataset.org/#download\n",
    "class_names = [\"car\", \"motorcycle\", \"bus\", \"truck\"]\n",
    "dir_name = \"coco_train\"\n",
    "data_utils.download_coco(anno_file, class_names, dir_name, num_samples=-1)  # Use num_samples=-1 to download all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the generation/download was successful.\n",
    "data_utils.check_darknet_yolov4_annos(\"coco/coco_train\", \"v_det_class_name.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco_train dataset done, files saved:\n",
      "coco/train_coco_train.txt\n",
      "coco/val_coco_train.txt\n",
      "coco/test_coco_train.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train, val, and test sets.\n",
    "\n",
    "imgs_root = \"coco\"\n",
    "imgs_prefix = \"../../../../../datasets/coco\"\n",
    "\n",
    "data_utils.gen_dataset_split_files(\"coco_train\", imgs_root, imgs_prefix, train=100, val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n",
      "COCO categories: \n",
      "person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush\n",
      "\n",
      "Class: car\n",
      "Class ID: 3\n",
      "Number of imgs: 535\n",
      "Example img: {'license': 3, 'file_name': '000000511999.jpg', 'coco_url': 'http://images.cocodataset.org/val2017/000000511999.jpg', 'height': 446, 'width': 640, 'date_captured': '2013-11-17 10:43:04', 'flickr_url': 'http://farm9.staticflickr.com/8246/8647068737_1f58d52a62_z.jpg', 'id': 511999} \n",
      "\n",
      "Class: motorcycle\n",
      "Class ID: 4\n",
      "Number of imgs: 159\n",
      "Example img: {'license': 1, 'file_name': '000000068093.jpg', 'coco_url': 'http://images.cocodataset.org/val2017/000000068093.jpg', 'height': 480, 'width': 640, 'date_captured': '2013-11-15 12:39:06', 'flickr_url': 'http://farm5.staticflickr.com/4120/4803479594_4fea166ab0_z.jpg', 'id': 68093} \n",
      "\n",
      "Class: bus\n",
      "Class ID: 6\n",
      "Number of imgs: 189\n",
      "Example img: {'license': 3, 'file_name': '000000338428.jpg', 'coco_url': 'http://images.cocodataset.org/val2017/000000338428.jpg', 'height': 366, 'width': 640, 'date_captured': '2013-11-17 13:18:38', 'flickr_url': 'http://farm9.staticflickr.com/8012/7535177488_ac1b53c80f_z.jpg', 'id': 338428} \n",
      "\n",
      "Class: truck\n",
      "Class ID: 8\n",
      "Number of imgs: 250\n",
      "Example img: {'license': 3, 'file_name': '000000001532.jpg', 'coco_url': 'http://images.cocodataset.org/val2017/000000001532.jpg', 'height': 480, 'width': 640, 'date_captured': '2013-11-16 14:10:20', 'flickr_url': 'http://farm7.staticflickr.com/6074/6024384901_2932d35bce_z.jpg', 'id': 1532} \n",
      "\n",
      "Class IDs [3, 4, 6, 8]\n",
      "Total imgs: 1133 (including duplicates)\n",
      "100 images downloaded after 0.98 minutes\n",
      "200 images downloaded after 1.98 minutes\n",
      "300 images downloaded after 3.02 minutes\n",
      "400 images downloaded after 4.04 minutes\n",
      "500 images downloaded after 5.06 minutes\n",
      "600 images downloaded after 6.09 minutes\n",
      "700 images downloaded after 7.15 minutes\n",
      "800 images downloaded after 8.14 minutes\n",
      "900 images downloaded after 9.12 minutes\n",
      "1000 images downloaded after 10.1 minutes\n",
      "1100 images downloaded after 11.08 minutes\n",
      "Finished downloading all images and creating annotation files for ['car', 'motorcycle', 'bus', 'truck'] classes in\n",
      " coco/coco_val\n"
     ]
    }
   ],
   "source": [
    "# Downloading the COCO images (validation set)\n",
    "anno_file = \"coco/instances_val2017.json\"  # Get this from https://cocodataset.org/#download\n",
    "class_names = [\"car\", \"motorcycle\", \"bus\", \"truck\"]\n",
    "dir_name = \"coco_val\"\n",
    "data_utils.download_coco(anno_file, class_names, dir_name, num_samples=-1)  # Use num_samples=-1 to download all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the generation/download was successful.\n",
    "data_utils.check_darknet_yolov4_annos(\"coco/coco_val\", \"v_det_class_name.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco_val dataset done, files saved:\n",
      "coco/train_coco_val.txt\n",
      "coco/val_coco_val.txt\n",
      "coco/test_coco_val.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train, val, and test sets.\n",
    "imgs_root = \"coco\"\n",
    "imgs_prefix = \"../../../../../datasets/coco\"\n",
    "\n",
    "data_utils.gen_dataset_split_files(\"coco_val\", imgs_root, imgs_prefix, train=0, val=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all annotations in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All annotations saved in all_annos.json.\n"
     ]
    }
   ],
   "source": [
    "data_utils.save_all_annos_as_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
